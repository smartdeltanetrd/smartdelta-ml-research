{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cafba99-0e9b-449c-bb77-f0678d62af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import dateutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a6564-7470-4648-97ae-0c92b269f539",
   "metadata": {},
   "source": [
    "# Logs comparison\n",
    "\n",
    "This is \"use case b\" from the list of planned use cases:\n",
    "1. (a). **Trends monitoring**. User specifies log fields to monitor and specifies their min/max/alert levels. Tool makes prognosis and form summary for these log fields (and also for system load, e.g. number of messages per hour). The prognosis is based on previous dynamics, previous/future working days, holidays, and other events (like maintenance windows).\n",
    "1. (b). **Logs comparison**. User compares current logs with the previous ones (e.g. from previous release). User selects log fields to analyze. Tool highlights high-level differences, like number of messages, differences in prev/next hops, maybe different trends of field values.\n",
    "1. (c). **Anomalies in logs**. Tool tries to find messages, which donâ€™t look similar to most of others (for example, less than 1%). One more case: cluster log messages, if we see several types of them.\n",
    "1. (d). **Automatic fault detection**. Tool automatically finds and highlight failures, basing on HTTP codes and, probably, other fields.\n",
    "1. (e). **Failure patterns**. Using the data from automatic failure detection module, tool tries to find any pattern in failures, like occurring only on 5th time after connection setup, also it tries to find precursors to failure (certain messages or values, which appear before it happens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3ea15-4cf1-4422-bad4-4e4f21b77fdf",
   "metadata": {},
   "source": [
    "## Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c0265a-2697-40a0-9391-10ff71ea854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldBaseType = enum.Enum('FieldBaseType', 'float integer categorical date string')\n",
    "\n",
    "class FieldType_General:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        if not isinstance(data, np.ndarray) or len(data.shape) != 1:\n",
    "            raise ValueError(f\"Expected 1-d numpy array, got: {type(data)}\")\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "class IntOrFloatMixin:\n",
    "\n",
    "    def find_big_differences(self, other_field):\n",
    "        BIG_DIFF_THR = 2  # We suppose that big diffs shold be greater than BIG_DIFF_THR*diff.mean()\n",
    "        RARE_DIFF_THR = 0.1  # We suppose that rare diffs should occur less than RARE_DIFF_THR*len(data)\n",
    "        MAX_THR_CHANGE_ITERS = 10  # No more than 10 iterations to find optimal threshold\n",
    "\n",
    "        other_data = other_field.data\n",
    "        if len(self.data) != len(other_data):\n",
    "            raise ValueError(f\"Lengths are not equal: {len(self.data)} and {len(other_data)}\")\n",
    "        diff = np.abs(self.data - other_data)\n",
    "        if np.isclose(diff.mean(), 0) or diff.max() <= BIG_DIFF_THR*diff.mean():\n",
    "            return np.array([], dtype=int)\n",
    "        diff_vals = np.unique(diff)\n",
    "        if len(diff_vals) < 2:\n",
    "            return np.array([], dtype=int)\n",
    "        biggest_thr = diff_vals[-2]  # one value before the maximum\n",
    "\n",
    "        big_thr = BIG_DIFF_THR*diff.mean()\n",
    "        if big_thr >= biggest_thr:\n",
    "            return np.array([], dtype=int)\n",
    "        found = False\n",
    "        for thr in np.linspace(big_thr, biggest_thr, num=MAX_THR_CHANGE_ITERS):\n",
    "            if len(np.where(diff > thr)) < RARE_DIFF_THR*len(diff):\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            times = np.argwhere(diff > thr)  # thr is still defined after loop\n",
    "        else:\n",
    "            times = np.array([], dtype=int)\n",
    "        return times\n",
    "\n",
    "\n",
    "class FieldType_Float(FieldType_General, IntOrFloatMixin):\n",
    "    BASE_TYPE = FieldBaseType.float\n",
    "\n",
    "\n",
    "class FieldType_Int(FieldType_General, IntOrFloatMixin):\n",
    "    BASE_TYPE = FieldBaseType.integer\n",
    "\n",
    "\n",
    "class FieldType_Cat(FieldType_General):\n",
    "    BASE_TYPE = FieldBaseType.categorical\n",
    "\n",
    "    def compare_categories(self, other_field):\n",
    "        other_data = other_field.data\n",
    "        if len(self.data) != len(other_data):\n",
    "            raise ValueError(f\"Lengths are not equal: {len(self.data)} and {len(other_data)}\")\n",
    "        \n",
    "        diff = np.where(self.data == other_data, 0, 1)\n",
    "        match_rel = np.count_nonzero(diff == 0)/len(diff)\n",
    "        matched_vals = set(self.data[np.argwhere(diff == 0)].flatten())\n",
    "        nonmatched_vals = set(self.data[np.argwhere(diff != 0)].flatten()) | set(other_data[np.argwhere(diff != 0)].flatten())\n",
    "        strict_matched_vals = matched_vals - nonmatched_vals\n",
    "        strict_nonmatched_vals = nonmatched_vals - matched_vals\n",
    "        return match_rel, matched_vals, nonmatched_vals, strict_matched_vals, strict_nonmatched_vals\n",
    "\n",
    "    def count_values(self):\n",
    "        vals, counts = np.unique(self.data, return_counts=True)\n",
    "        cnt_idxs = counts.argsort()\n",
    "        return vals[cnt_idxs[::-1]], counts[cnt_idxs[::-1]]\n",
    "\n",
    "\n",
    "class FieldType_Date(FieldType_General):\n",
    "    BASE_TYPE = FieldBaseType.date\n",
    "\n",
    "    def __init__(self, data):\n",
    "        time_data = np.vectorize(lambda t: t.timestamp())(data)\n",
    "        super().__init__(time_data)\n",
    "\n",
    "\n",
    "class FieldType_Str(FieldType_General):\n",
    "    BASE_TYPE = FieldBaseType.string\n",
    "\n",
    "\n",
    "class FieldType_Resource(FieldType_Float):\n",
    "\n",
    "    def __init__(self, data, low_val, high_val, low_warn_level, high_warn_level):\n",
    "        super().__init__(data)\n",
    "        self.low_val = low_val\n",
    "        self.high_val = high_val\n",
    "        self.low_warn_level = low_warn_level\n",
    "        self.high_warn_level = high_warn_level\n",
    "\n",
    "\n",
    "class FieldType_CPUUtilization(FieldType_Resource):\n",
    "\n",
    "    def __init__(self, data, high_warn_level):\n",
    "        super().__init__(data, 0, 100, None, high_warn_level)\n",
    "\n",
    "\n",
    "class FieldType_RAMUtilization(FieldType_Resource):\n",
    "\n",
    "    def __init__(self, data, low_warn_level, high_warn_level):\n",
    "        super().__init__(data, 0, 100, low_warn_level, high_warn_level)\n",
    "\n",
    "\n",
    "def create_field_object(field_s, name, verbose=True):\n",
    "    obj = None\n",
    "    if obj is None:\n",
    "        if np.issubdtype(field_s.dtype, np.floating):\n",
    "            obj = FieldType_Float(field_s.values)\n",
    "        elif np.issubdtype(field_s.dtype, np.integer):\n",
    "            # TODO: may be categorical?\n",
    "            obj = FieldType_Int(field_s.values)\n",
    "    if obj is None:\n",
    "        # here we assume, that it is string, but it also can be categorical\n",
    "        try:\n",
    "            date_s = field_s.apply(dateutil.parser.parse)\n",
    "            obj = FieldType_Date(date_s.values)\n",
    "        except:\n",
    "            pass\n",
    "    if obj is None:\n",
    "        try:\n",
    "            float_s = field_s.apply(float)\n",
    "            try:\n",
    "                int_s = float_s.apply(int)\n",
    "                obj = FieldType_Int(int_s.values)\n",
    "            except:\n",
    "                obj = FieldType_Float(float_s.values)\n",
    "        except:\n",
    "            pass\n",
    "    if obj is None:\n",
    "        if field_s.nunique() < 0.9*len(field_s):\n",
    "            obj = FieldType_Cat(field_s.values)\n",
    "        else:\n",
    "            obj = FieldType_Str(field_s.values)\n",
    "    if verbose:\n",
    "        print(f\"{name}: autodetected type is {obj.BASE_TYPE}\")\n",
    "    return obj\n",
    "\n",
    "\n",
    "def comparable_types(field1, field2):\n",
    "    # TODO: implementation is missing\n",
    "    return True\n",
    "\n",
    "def align_base_field_types(field1, field2):\n",
    "    # TODO: implementation is missing\n",
    "    return field1, field2\n",
    "\n",
    "\n",
    "def compare_fields(field1: FieldType_General, field2: FieldType_General):\n",
    "    field1, field2 = align_base_field_types(field1, field2)\n",
    "    comparison_res = {}\n",
    "    if issubclass(type(field1), IntOrFloatMixin):\n",
    "        comparison_res['big_difference_idxs'] = field1.find_big_differences(field2)\n",
    "    if issubclass(type(field1), FieldType_Cat):\n",
    "        match_rel, matched_vals, nonmatched_vals, strict_matched_vals, strict_nonmatched_vals = field1.compare_categories(field2)\n",
    "        comparison_res['matched_relation'] = match_rel\n",
    "        comparison_res['matched_vals'] = matched_vals\n",
    "        comparison_res['nonmatched_vals'] = nonmatched_vals\n",
    "        comparison_res['strict_matched_vals'] = strict_matched_vals\n",
    "        comparison_res['strict_nonmatched_vals'] = strict_nonmatched_vals\n",
    "    return comparison_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a2ad58-802d-4b95-9059-46ddd95602c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageList:\n",
    "\n",
    "    last_list_id = 1\n",
    "\n",
    "    def __init__(self, init_df, name=\"\", verbose=True):\n",
    "        self.fields = {col: create_field_object(init_df[col], col, verbose=verbose) for col in init_df.columns.tolist()}\n",
    "        if len(name) == 0:\n",
    "            name = f\"message_list_{MessageList.last_list_id}\"\n",
    "            MessageList.last_list_id += 1\n",
    "        self.name = name\n",
    "\n",
    "    def get_categorical_fields(self):\n",
    "        return {f_name: f_field for f_name, f_field in self.fields.items() if isinstance(f_field, FieldType_Cat)}\n",
    "\n",
    "    def categorical_combinations_stat(self, field_names=None):\n",
    "        # same results can be obtained by Pandas \"value_counts\" dataframe function, but here we have more control on processing\n",
    "        res_stat = {'combinations': None, 'counts': None, 'next_level': None}\n",
    "        if field_names is None:\n",
    "            field_names = list(self.get_categorical_fields().keys())\n",
    "        if len(field_names) < 1:\n",
    "            return res_stat\n",
    "\n",
    "        current_stat = res_stat\n",
    "        level_idxs = []\n",
    "        level_values = []\n",
    "        prev_stats = []\n",
    "        current_level = 0\n",
    "        going_forward = True\n",
    "        all_idxs = np.arange(0, len(self.fields[field_names[0]].data), 1, dtype=int)\n",
    "        while True:\n",
    "            # print(f\"combs: {current_level = } \" + \" \".join([f\"{li}/{(len(lv) - 1)}\" for li, lv in zip(level_idxs, level_values)]))\n",
    "            if current_level > 0:\n",
    "                msg_idx = all_idxs\n",
    "                for i in range(0, current_level):\n",
    "                    new_msg_idx = np.nonzero(self.fields[field_names[i]].data[msg_idx] == level_values[i][level_idxs[i]])\n",
    "                    skipped_elems = np.setdiff1d(msg_idx, all_idxs, assume_unique=True)\n",
    "                    for s_elem in np.sort(skipped_elems):\n",
    "                        new_msg_idx[new_msg_idx > s_elem] += 1\n",
    "                    msg_idx = new_msg_idx\n",
    "                filtered_data = self.fields[field_names[current_level]].data[msg_idx]\n",
    "            else:\n",
    "                filtered_data = self.fields[field_names[current_level]].data\n",
    "\n",
    "            # on the first pass data length must be > 0, but on the next passes it can be 0\n",
    "            assert len(filtered_data) > 0 or current_level < len(level_idxs)\n",
    "\n",
    "            if len(filtered_data) > 0:\n",
    "                values, counts = np.unique(filtered_data, return_counts=True)\n",
    "                cnt_idxs = counts.argsort()\n",
    "                values = values[cnt_idxs[::-1]].tolist()\n",
    "                counts = counts[cnt_idxs[::-1]].tolist()\n",
    "    \n",
    "                if len(level_idxs) <= current_level:\n",
    "                    level_idxs.append(0)\n",
    "                    level_values.append(values)\n",
    "                    assert len(level_idxs) == current_level + 1\n",
    "    \n",
    "                    hist_list = [level_values[i][level_idxs[i]] for i in range(0, current_level)]\n",
    "                    current_stat['combinations'] = [hist_list + [v] for v in values]\n",
    "                    current_stat['counts'] = counts\n",
    "                else:\n",
    "                    hist_list = [level_values[i][level_idxs[i]] for i in range(0, current_level)]\n",
    "                    assert hist_list + [values[0]] not in current_stat['combinations']\n",
    "                    current_stat['combinations'] += [hist_list + [v] for v in values]\n",
    "                    current_stat['counts'] += counts\n",
    "\n",
    "            if current_level == len(field_names) - 1:\n",
    "                going_forward = False\n",
    "\n",
    "            if not going_forward:\n",
    "                level_idxs[current_level] += 1\n",
    "                exit_flag = False\n",
    "                while level_idxs[current_level] > len(level_values[current_level]) - 1:\n",
    "                    if current_level == 0:\n",
    "                        exit_flag = True\n",
    "                        break\n",
    "                    level_idxs[current_level] = 0\n",
    "                    level_idxs[current_level - 1] += 1\n",
    "                    current_level -= 1\n",
    "                    current_stat = prev_stats.pop(-1)\n",
    "                if exit_flag:\n",
    "                    break\n",
    "                going_forward = True\n",
    "\n",
    "            current_level += 1\n",
    "            if len(level_idxs) <= current_level:\n",
    "                current_stat['next_level'] = {'combinations': None, 'counts': None, 'next_level': None}\n",
    "            prev_stats.append(current_stat)\n",
    "            current_stat = current_stat['next_level']\n",
    "\n",
    "        return res_stat, field_names\n",
    "\n",
    "    def categorical_clusters_from_stat(self, categorical_stat, field_names):\n",
    "        MAX_CLUSTER_COUNT = 4\n",
    "        CLUSTERS_MIN_PERCENT = 0.7\n",
    "\n",
    "        found_clusters = []\n",
    "        current_stat = categorical_stat\n",
    "        for _ in range(len(field_names)):\n",
    "            level_stat = {'cluster_combinations': [], 'cluster_vals': [], 'noise_combinations': [], 'noise_vals': [], 'total_count': None}\n",
    "\n",
    "            assert len(set(tuple(l) for l in current_stat['combinations'])) == len(current_stat['combinations'])\n",
    "            if len(current_stat['counts']) > 1:\n",
    "                cluster_probs = np.array(current_stat['counts']) / sum(current_stat['counts'])\n",
    "                cluster_idxs = np.argsort(cluster_probs)[::-1]\n",
    "                # subtract sum of smallest clusters from sum of the biggest ones\n",
    "                # (last element is not needed, because in this case either smallest or biggest cluster is absent)\n",
    "                big_clusters_sum = np.cumsum(cluster_probs[cluster_idxs[:-1]])\n",
    "                small_clusters_sum = np.cumsum(cluster_probs[cluster_idxs[::-1][:-1]])[::-1]\n",
    "                clusters_percent = big_clusters_sum - small_clusters_sum\n",
    "                big_clusters_idxs = np.argwhere(clusters_percent[:MAX_CLUSTER_COUNT] >= CLUSTERS_MIN_PERCENT).flatten()\n",
    "                if len(big_clusters_idxs) > 0:\n",
    "                    # since we search for the smallest combination of biggest clusters, we take only first index of\n",
    "                    # big_clusters_idxs, which corresponds to the sum of precents of biggest clusters\n",
    "                    found_cluster_idxs = cluster_idxs[:big_clusters_idxs[0] + 1]\n",
    "                else:\n",
    "                    # corresponds to big amount of rare enough combinations\n",
    "                    found_cluster_idxs = np.array([])\n",
    "            else:\n",
    "                # single cluster is not interesting\n",
    "                found_cluster_idxs = np.array([])\n",
    "\n",
    "            assert len(current_stat['counts']) == len(current_stat['combinations'])\n",
    "            assert all([len(c) == _ + 1 for c in current_stat['combinations']])\n",
    "            level_stat['cluster_combinations'] = [comb for i, comb in enumerate(current_stat['combinations']) if i in found_cluster_idxs]\n",
    "            level_stat['cluster_vals'] = [cnt for i, cnt in enumerate(current_stat['counts']) if i in found_cluster_idxs]\n",
    "            level_stat['noise_combinations'] = [comb for i, comb in enumerate(current_stat['combinations']) if i not in found_cluster_idxs]\n",
    "            level_stat['noise_vals'] = [cnt for i, cnt in enumerate(current_stat['counts']) if i not in found_cluster_idxs]\n",
    "            level_stat['total_count'] = sum(current_stat['counts'])\n",
    "            found_clusters.append(level_stat)\n",
    "\n",
    "            current_stat = current_stat['next_level']\n",
    "\n",
    "        return found_clusters\n",
    "\n",
    "    def print_categorical_clusters(self, found_cluster_stat, field_names):\n",
    "        MAX_NOISE_COMBS = 3\n",
    "        cluster_found = False\n",
    "        for stat in found_cluster_stat:\n",
    "            cluster_count = len(stat['cluster_combinations'])\n",
    "            if cluster_count == 0:\n",
    "                continue\n",
    "            if not cluster_found:\n",
    "                print(\"The following clusters found:\")\n",
    "                cluster_found = True\n",
    "            noun_count_str = \"s\" if cluster_count > 1 else \"\"\n",
    "            verb_count_str = \"\" if cluster_count > 1 else \"s\"\n",
    "            print(f\"   The following {len(stat['cluster_combinations'])} message instance{noun_count_str} \" +\n",
    "                  f\"take{verb_count_str} {100*sum(stat['cluster_vals'])/stat['total_count']:00.1f}% of all messages:\")\n",
    "            for i, comb in enumerate(stat['cluster_combinations']):\n",
    "                print(f\"        [{100*stat['cluster_vals'][i]/stat['total_count']:00.1f}%]:\")\n",
    "                for j, value in enumerate(comb):\n",
    "                    print(f\"            {field_names[j]}:\\t\\t{value}\")\n",
    "\n",
    "            noun_count_str = \"s\" if len(stat['noise_combinations']) > 1 else \"\"\n",
    "            if len(stat['noise_combinations']) > MAX_NOISE_COMBS:\n",
    "                noise_header = f\"Fisrt {MAX_NOISE_COMBS} of them:\"\n",
    "            else:\n",
    "                noise_header = \"All of them:\"\n",
    "            print(f\"    There are {len(stat['noise_combinations'])} message instance{noun_count_str}, \" +\n",
    "                  f\"which occur in the rest of cases. {noise_header}\")\n",
    "            biggest_combs_idxs = np.argsort(stat['noise_vals'])[::-1][:MAX_NOISE_COMBS]\n",
    "            sorted_noise_combs = list(np.array(stat['noise_combinations'])[biggest_combs_idxs])\n",
    "            sorted_noise_vals = list(np.array(stat['noise_vals'])[biggest_combs_idxs])\n",
    "            for comb, count in zip(sorted_noise_combs, sorted_noise_vals):\n",
    "                print(f\"        [{100*count/stat['total_count']:00.1f}%]:\")\n",
    "                for j, value in enumerate(comb):\n",
    "                    print(f\"            {field_names[j]}:\\t\\t{value}\")\n",
    "            print(\"    -----------------------------------------\")\n",
    "        if not cluster_found:\n",
    "            print(\"No clusters found\")\n",
    "        else:\n",
    "            print(\"=============================================\")\n",
    "\n",
    "\n",
    "def compare_messages_by_categorical_fields(msg_list1, msg_list2, cols_to_compare=None, verbose=True):\n",
    "    if cols_to_compare is None:\n",
    "        cols_to_compare = set()\n",
    "        for f1_name in msg_list1.get_categorical_fields().keys():\n",
    "            if f1_name in msg_list2.get_categorical_fields().keys():\n",
    "                    cols_to_compare.add(f1_name)\n",
    "        if len(cols_to_compare) == 0:\n",
    "            if verbose:\n",
    "                print(\"No fields matched to compare messages!\")\n",
    "            return None\n",
    "        if verbose and len(cols_to_compare) < len(msg_list1.keys()):\n",
    "            print(f\"The following columns from \\\"{msg_list1.name}\\\" did't match with \\\"{msg_list2.name}\\\" by type and will be ignored: \"\n",
    "                  f\"{set(msg_list1.keys()) - cols_to_compare}\")\n",
    "        if verbose and len(cols_to_compare) < len(msg_list2.keys()):\n",
    "            print(f\"The following columns from \\\"{msg_list2.name}\\\" did't match with \\\"{msg_list1.name}\\\" by type and will be ignored: \"\n",
    "                  f\"{set(msg_list2.keys()) - cols_to_compare}\")\n",
    "    # for col in cols_to_compare:   \n",
    "    #     count_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d8688-9926-4c14-a478-405ed4844aa1",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "There are two ways how to compare messages in the logs (at least what comes to mind):\n",
    "1. \"Vertical comparison\": two log files are compared \"field by field\" for all messages. So fields of the same type (in highest-level meaning of \"type\", of course: including field name, etc.) are compared for all messages, throughout whole files. This means that each time we compare two time series, where time values can be different.\n",
    "1. \"Horizontal comparison\": messages are separately clustered by instances in each log file, then found clusters are compared with each other.\n",
    "\n",
    "Notes on \"horizontal clustering\" (by messages):\n",
    "1. It is not possible to clusterize messages by categorical and continuous fields without metainformation: categorical and continuous distances should have weights to be compared.\n",
    "1. Categorical clustering depends on field order, so it should be defined in advance, othervise results will be randem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc8e9fb-90fb-4e44-a905-60c7a0296ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@timestamp: autodetected type is FieldBaseType.string\n",
      "Class: autodetected type is FieldBaseType.categorical\n",
      "Class.keyword: autodetected type is FieldBaseType.categorical\n",
      "Device: autodetected type is FieldBaseType.categorical\n",
      "Level: autodetected type is FieldBaseType.categorical\n",
      "Line: autodetected type is FieldBaseType.integer\n",
      "Message: autodetected type is FieldBaseType.string\n",
      "Method: autodetected type is FieldBaseType.categorical\n",
      "Pod: autodetected type is FieldBaseType.categorical\n",
      "Service: autodetected type is FieldBaseType.categorical\n",
      "Subscriber: autodetected type is FieldBaseType.categorical\n",
      "Thread: autodetected type is FieldBaseType.categorical\n",
      "_id: autodetected type is FieldBaseType.string\n",
      "_index: autodetected type is FieldBaseType.categorical\n",
      "_score: autodetected type is FieldBaseType.categorical\n",
      "_type: autodetected type is FieldBaseType.categorical\n",
      "@timestamp: autodetected type is FieldBaseType.string\n",
      "Class: autodetected type is FieldBaseType.categorical\n",
      "Class.keyword: autodetected type is FieldBaseType.categorical\n",
      "Device: autodetected type is FieldBaseType.categorical\n",
      "Level: autodetected type is FieldBaseType.categorical\n",
      "Line: autodetected type is FieldBaseType.integer\n",
      "Message: autodetected type is FieldBaseType.string\n",
      "Method: autodetected type is FieldBaseType.categorical\n",
      "Pod: autodetected type is FieldBaseType.categorical\n",
      "Service: autodetected type is FieldBaseType.categorical\n",
      "Subscriber: autodetected type is FieldBaseType.categorical\n",
      "Thread: autodetected type is FieldBaseType.categorical\n",
      "_id: autodetected type is FieldBaseType.string\n",
      "_index: autodetected type is FieldBaseType.categorical\n",
      "_score: autodetected type is FieldBaseType.categorical\n",
      "_type: autodetected type is FieldBaseType.categorical\n",
      "{'big_difference_idxs': array([], dtype=int64)}\n",
      "{'matched_relation': 0.0, 'matched_vals': set(), 'nonmatched_vals': {'logging-000039', 'deserializeMessage', 'sendLocalMessage'}, 'strict_matched_vals': set(), 'strict_nonmatched_vals': {'logging-000039', 'deserializeMessage', 'sendLocalMessage'}}\n",
      "['Class', 'Class.keyword', 'Device', 'Level', 'Method', 'Pod', 'Service', 'Subscriber', 'Thread', '_index', '_score', '_type']\n",
      "The following clusters found:\n",
      "   The following 3 message instances take 85.2% of all messages:\n",
      "        [39.5%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\t${ctx:device}\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tdeserializeMessage\n",
      "        [23.5%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\t${ctx:device}\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tsendLocalMessage\n",
      "        [22.2%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser01-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tdeserializeMessage\n",
      "    There are 1 message instance, which occur in the rest of cases. All of them:\n",
      "        [14.8%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser01-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tsendLocalMessage\n",
      "    -----------------------------------------\n",
      "=============================================\n",
      "The following clusters found:\n",
      "   The following 2 message instances take 90.8% of all messages:\n",
      "        [49.2%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\t${ctx:device}\n",
      "        [41.5%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser01-fake.email-0001\n",
      "    There are 1 message instance, which occur in the rest of cases. All of them:\n",
      "        [9.2%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser02-fake.email-0001\n",
      "    -----------------------------------------\n",
      "   The following 2 message instances take 90.8% of all messages:\n",
      "        [49.2%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\t${ctx:device}\n",
      "            Level:\t\tINTERNALTRACE\n",
      "        [41.5%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser01-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "    There are 1 message instance, which occur in the rest of cases. All of them:\n",
      "        [9.2%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser02-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "    -----------------------------------------\n",
      "   The following 4 message instances take 90.8% of all messages:\n",
      "        [24.6%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\t${ctx:device}\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tsendLocalMessage\n",
      "        [24.6%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\t${ctx:device}\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tdeserializeMessage\n",
      "        [21.5%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser01-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tsendLocalMessage\n",
      "        [20.0%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser01-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tdeserializeMessage\n",
      "    There are 2 message instances, which occur in the rest of cases. All of them:\n",
      "        [4.6%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser02-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tdeserializeMessage\n",
      "        [4.6%]:\n",
      "            Class:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Class.keyword:\t\tcom.genband.util.broker.util.MessageFactory\n",
      "            Device:\t\tgaap-nvs-cca-gaapwebrtcuser02-fake.email-0001\n",
      "            Level:\t\tINTERNALTRACE\n",
      "            Method:\t\tsendLocalMessage\n",
      "    -----------------------------------------\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "logs1_df = pd.read_csv('data/chatbroker.csv')\n",
    "logs2_df = pd.read_csv('data/smsbroker.csv')\n",
    "\n",
    "logs1_df = logs1_df[:len(logs2_df)]  # temporary hack\n",
    "\n",
    "msgs1 = MessageList(logs1_df, verbose=True)\n",
    "msgs2 = MessageList(logs2_df, verbose=True)\n",
    "\n",
    "res = compare_fields(msgs1.fields['Line'], msgs2.fields['Line'])\n",
    "print(res)\n",
    "res = compare_fields(msgs1.fields['_index'], msgs2.fields['Method'])\n",
    "print(res)\n",
    "\n",
    "stat, cat_field_names = msgs1.categorical_combinations_stat()\n",
    "print(cat_field_names)\n",
    "# print(json.dumps(stat, indent=4))\n",
    "# print(stat)\n",
    "cat_clusters = msgs1.categorical_clusters_from_stat(stat, cat_field_names)\n",
    "# print(json.dumps(cat_clusters, indent=4))\n",
    "msgs1.print_categorical_clusters(cat_clusters, cat_field_names)\n",
    "\n",
    "stat, cat_field_names = msgs2.categorical_combinations_stat()\n",
    "cat_clusters = msgs2.categorical_clusters_from_stat(stat, cat_field_names)\n",
    "msgs2.print_categorical_clusters(cat_clusters, cat_field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f940ef-a9cb-4b3f-8a74-917cfb15b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field by field comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1110a4-262c-4186-914a-290961729cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message statistics comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
